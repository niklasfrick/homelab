toolbox:
  enabled: true
  resources:
    limits:
      memory: 256Mi
    requests:
      cpu: 20m
      memory: 128Mi

monitoring:
  enabled: true
  createPrometheusRules: true

cephClusterSpec:
  cephConfig:
    global:
      bdev_enable_discard: "true"
      bdev_async_discard_threads: "1"
  crashCollector:
    disable: true
  dashboard:
    ssl: false
  storage:
    useAllDevices: false
    devices:
      - name: "sda5"
      - name: "nvme0n1p5"
  resources:
    mgr:
      limits:
        memory: "1Gi"
      requests:
        cpu: "30m"
        memory: "512Mi"
    mon:
      limits:
        memory: "1Gi"
      requests:
        cpu: "35m"
        memory: "512Mi"
    osd:
      limits:
        memory: "1520Mi"
      requests:
        cpu: "30m"
        memory: "1322Mi"
    prepareosd:
      requests:
        cpu: "200m"
    cleanup:
      requests:
        cpu: "200m"
        memory: "50Mi"
    logCollector:
      requests:
        cpu: "10m"
        memory: "100Mi"
      limits:
        memory: "100Mi"
    exporter:
      requests:
        cpu: "10m"
        memory: "100Mi"
      limits:
        memory: "250Mi"
  annotations:
    dashboard:
      omni-kube-service-exposer.sidero.dev/port: "51001"
      omni-kube-service-exposer.sidero.dev/label: "Ceph Dashboard"
      omni-kube-service-exposer.sidero.dev/icon: "H4sICM8nE2kAA2NlcGguc3ZnAG1V7W4bOQx8FWHv99ISSX0VcYD0fx9i4bRxAKftXY26yNN3Rmv3mm5hi5FFSiJnhsrdt+9P4cfL6fO3/XQ8n7++2+0ul4tcTL7897TTGOMOEVO4PD+ej/upxTiF48fnp+P5+uPT8+m0n/75FF39cQrfnz9e3n/5sZ9iiEEd3+n+7utyPobH/fQhJem5i+b6wGkYJo6PSY4GW8tY1auN109S0VSXP3cl0VhDFnU7SMUm8eqwrbWg0t1xosYWXKK7tDLDD7flWXpndLY5SfYsaZZsNksyx0qKdZZqFdMW89IFuQ0zLp2ZTJ+zdKuHOLv0Nq7IbUYQbhh2nR8ltoPztjiW18B+nY+Qvx7fAo8vB6TVu5SO1Bor067MLLNIQ55Nci6I4UIsjTU5K8tKNEyV1QALbx3n5joA6TMBKRsSQNhvQFtY7fDNg6C5AbKVvLcMYVt0EnucMUmvL/gTM2ptVpcqIGmY21FkGb/zQxUNHOspRVIOTWK35Y2DNRRWUXJ4uwOXoaKkXE9j/BKGMj7WA850ygRgFLEOsGIHYPUkqUE5UZkpRvlXIstAYgiMip3FMrBuEC1VA32MmYqVxYQX+5WxJHVVXAOeyu8VUCltMNOYhwOU1qRw4p4GBJRYbLZAIsF/EQHpgi2IpkgZY4VBnCRaxdI6buhYVDii2Ym4Kw3uSErjChEp9K0O2bs2HgN5F8bVrrg6Bb8ihzAvQzNG9myMqyMqW6NxXa9meAKKLGyj4lzV1awuQGQ8rlZjJ2RcNvy/u8NwHwTwegE9Q8URhOFYwl64649MjPr3Hja504HcUXKqkHpHydbZ4mvFLPhEZIzADJw4li2glBsA3TIQBgMbvsh+PzADKa58GvB7cL+KHRyjYVoY5n91rPzXsNUTqcbDggdGWoJ603iqOiRYKILcTxQpSjCKdy0lNTCu0SG2hheDYQMAyXzfelm2XTKzS9q2rWa2lYdtH87bNqT2nOLoY8e22ykBtDsfh/76Ae2BMurCRsphtbdzrOhYSQ+0HlZ7e2RCcryFaey0sNobljZgKX/fh/tKe51293f8f3b/E9Jto233BgAA"

cephBlockPools:
  - name: ceph-blockpool
    # see https://github.com/rook/rook/blob/v1.19.0-beta.0/Documentation/CRDs/Block-Storage/ceph-block-pool-crd.md#spec for available configuration
    spec:
      failureDomain: host
      replicated:
        size: 3
      # Enables collecting RBD per-image IO statistics by enabling dynamic OSD performance counters. Defaults to false.
      # For reference: https://docs.ceph.com/docs/latest/mgr/prometheus/#rbd-io-statistics
      # enableRBDStats: true
    storageClass:
      enabled: true
      name: ceph-block
      annotations: {}
      labels: {}
      isDefault: true
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"
      mountOptions: []
      # see https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies
      allowedTopologies: []
      #        - matchLabelExpressions:
      #            - key: rook-ceph-role
      #              values:
      #                - storage-node
      # see https://github.com/rook/rook/blob/v1.19.0-beta.0/Documentation/Storage-Configuration/Block-Storage-RBD/block-storage.md#provision-storage for available configuration
      parameters:
        # (optional) mapOptions is a comma-separated list of map options.
        # For krbd options refer
        # https://docs.ceph.com/docs/latest/man/8/rbd/#kernel-rbd-krbd-options
        # For nbd options refer
        # https://docs.ceph.com/docs/latest/man/8/rbd-nbd/#options
        # mapOptions: lock_on_read,queue_depth=1024

        # (optional) unmapOptions is a comma-separated list of unmap options.
        # For krbd options refer
        # https://docs.ceph.com/docs/latest/man/8/rbd/#kernel-rbd-krbd-options
        # For nbd options refer
        # https://docs.ceph.com/docs/latest/man/8/rbd-nbd/#options
        # unmapOptions: force

        # RBD image format. Defaults to "2".
        imageFormat: "2"

        # RBD image features, equivalent to OR'd bitfield value: 63
        # Available for imageFormat: "2". Older releases of CSI RBD
        # support only the `layering` feature. The Linux kernel (KRBD) supports the
        # full feature complement as of 5.4
        imageFeatures: layering

        # These secrets contain Ceph admin credentials.
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/controller-publish-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-publish-secret-namespace: "{{ .Release.Namespace }}"
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
        # Specify the filesystem type of the volume. If not specified, csi-provisioner
        # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
        # in hyperconverged settings where the volume is mounted on the same node as the osds.
        csi.storage.k8s.io/fstype: ext4

cephFileSystems:
  - name: ceph-filesystem
    # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
    spec:
      metadataPool:
        replicated:
          size: 3
      dataPools:
        - failureDomain: host
          replicated:
            size: 3
          # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
          name: data0
      metadataServer:
        activeCount: 1
        activeStandby: true
        resources:
          limits:
            memory: "100Mi"
          requests:
            cpu: "20m"
            memory: "100Mi"
        priorityClassName: system-cluster-critical
    storageClass:
      enabled: true
      isDefault: false
      name: ceph-filesystem
      # (Optional) specify a data pool to use, must be the name of one of the data pools above, 'data0' by default
      pool: data0
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"
      annotations: {}
      labels: {}
      mountOptions: []
      # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage.md#provision-storage for available configuration
      parameters:
        # The secrets contain Ceph admin credentials.
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-publish-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-publish-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        # Specify the filesystem type of the volume. If not specified, csi-provisioner
        # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
        # in hyperconverged settings where the volume is mounted on the same node as the osds.
        csi.storage.k8s.io/fstype: ext4

cephObjectStores: []
