monitoring:
  enabled: true
  createPrometheusRules: true

cephClusterSpec:
  crashCollector:
    disable: true
  mon:
    count: 1 #TODO: remove once t3s-zendo-w3 is back
  mgr:
    count: 1 #TODO: remove once t3s-zendo-w3 is back
  storage:
    useAllDevices: false
    devices:
      - name: "sda5"
  resources:
    mgr:
      limits:
        memory: "1Gi"
      requests:
        cpu: "300m"
        memory: "512Mi"
    mon:
      limits:
        memory: "1Gi"
      requests:
        cpu: "300m"
        memory: "512Mi"
    osd:
      limits:
        memory: "2Gi"
      requests:
        cpu: "500m"
        memory: "1Gi"
    prepareosd:
      requests:
        cpu: "300m"
    cleanup:
      requests:
        cpu: "300m"

cephFileSystems:
  - name: ceph-filesystem
    # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
    spec:
      metadataPool:
        replicated:
          size: 3
      dataPools:
        - failureDomain: host
          replicated:
            size: 3
          # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
          name: data0
      metadataServer:
        activeCount: 1
        activeStandby: true
        resources:
          limits:
            memory: "1Gi"
          requests:
            cpu: "300m"
            memory: "512Mi"
        priorityClassName: system-cluster-critical
    storageClass:
      enabled: true
      isDefault: false
      name: ceph-filesystem
      # (Optional) specify a data pool to use, must be the name of one of the data pools above, 'data0' by default
      pool: data0
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"
      annotations: {}
      labels: {}
      mountOptions: []
      # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage.md#provision-storage for available configuration
      parameters:
        # The secrets contain Ceph admin credentials.
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-publish-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-publish-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        # Specify the filesystem type of the volume. If not specified, csi-provisioner
        # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
        # in hyperconverged settings where the volume is mounted on the same node as the osds.
        csi.storage.k8s.io/fstype: ext4

cephObjectStores:
  - name: ceph-objectstore
    # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Object-Storage/ceph-object-store-crd.md#object-store-settings for available configuration
    spec:
      metadataPool:
        failureDomain: host
        replicated:
          size: 3
      dataPool:
        failureDomain: host
        erasureCoded:
          dataChunks: 2
          codingChunks: 1
        parameters:
          bulk: "true"
      preservePoolsOnDelete: true
      gateway:
        port: 80
        resources:
          limits:
            memory: "2Gi"
          requests:
            cpu: "200m"
            memory: "512Mi"
        # securePort: 443
        # sslCertificateRef:
        instances: 1
        priorityClassName: system-cluster-critical
        # opsLogSidecar:
        #   resources:
        #     limits:
        #       memory: "100Mi"
        #     requests:
        #       cpu: "100m"
        #       memory: "40Mi"
    storageClass:
      enabled: true
      name: ceph-bucket
      reclaimPolicy: Delete
      volumeBindingMode: "Immediate"
      annotations: {}
      labels: {}
      # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Object-Storage-RGW/ceph-object-bucket-claim.md#storageclass for available configuration
      parameters:
        # note: objectStoreNamespace and objectStoreName are configured by the chart
        region: bal
    ingress:
      # Enable an ingress for the ceph-objectstore
      enabled: false
      # The ingress port by default will be the object store's "securePort" (if set), or the gateway "port".
      # To override those defaults, set this ingress port to the desired port.
      # port: 80
      # annotations: {}
      # host:
      #   name: objectstore.example.com
      #   path: /
      #   pathType: Prefix
      # tls:
      # - hosts:
      #     - objectstore.example.com
      #   secretName: ceph-objectstore-tls
      # ingressClassName: nginx
    route:
      # Enable an ingress for the ceph-objectstore
      enabled: false
      # The ingress port by default will be the object store's "securePort" (if set), or the gateway "port".
      # To override those defaults, set this ingress port to the desired port.
      # port: 80
      # annotations: {}
      # host:
      #   name: objectstore.example.com
      #   path: /
      #   pathType: PathPrefix
      # parentRefs:
      #   - name: internal
      #     namespace: kube-system
      #     sectionName: https
