servingEngineSpec:
  labels:
    environment: "spark"
    release: "v1"

  modelSpec:
    # ============================================================================
    # Mistral Large 3 - Multimodal MoE Model (41B active / 675B total params)
    # Optimized for NVIDIA DGX Spark with 128GB unified memory
    # ============================================================================
    # - name: "mistral-large-3"
    #   repository: "nvcr.io/nvidia/vllm"
    #   tag: "25.11-py3"
    #   modelURL: "mistralai/Mistral-Large-3-675B-Instruct-2512-NVFP4"
    #   replicaCount: 1

    #   vllmApiKey:
    #     secretName: "vllm-apikey"
    #     secretKEy: "VLLM_API_KEY"

    #   # DGX Spark: 10x ARM Cortex-X925 cores, 128GB unified memory, GB10 Blackwell GPU
    #   requestCPU: 8
    #   requestMemory: "100Gi"
    #   requestGPU: 1

    #   # Shared memory for MoE expert routing and tensor operations
    #   shmSize: "16Gi"

    #   # Persistent storage for model weights cache
    #   pvcStorage: "450Gi"
    #   pvcAccessMode:
    #     - ReadWriteOnce

    #   vllmConfig:
    #     # Context length - MoE model supports up to 256k but we use conservative value
    #     # for DGX Spark unified memory to leave room for KV cache
    #     maxModelLen: 65536

    #     gpuMemoryUtilization: 0.9

    #     # Auto dtype - will use optimal precision on Blackwell
    #     dtype: "auto"

    #     # Single GB10 GPU on DGX Spark
    #     tensorParallelSize: 1

    #     # Chunked prefill improves TTFT for long prompts
    #     enableChunkedPrefill: true

    #     # Prefix caching improves throughput for repeated prefixes
    #     enablePrefixCaching: true

    #     # Extra args optimized for DGX Spark unified memory + MoE architecture
    #     extraArgs:
    #       - "--max-num-batched-tokens=32768"
    #       - "--tokenizer-mode=mistral"
    #       - "--config-format=mistral"
    #       - "--load-format=mistral"
    #       - "--enable-auto-tool-choice"
    #       - "--tool-call-parser=mistral"

    #   nodeSelectorTerms:
    #     - matchExpressions:
    #         - key: nvidia.com/gpu.product
    #           operator: "In"
    #           values:
    #             - "NVIDIA-GB10"

    #   envFromSecret:
    #     name: hf-token

    #   - name: "gpt-oss-120b"
    #     repository: "nvcr.io/nvidia/vllm"
    #     tag: "25.11-py3"
    #     modelURL: "openai/gpt-oss-120b"
    #     replicaCount: 1

    #     vllmApiKey:
    #       secretName: "vllm-apikey"
    #       secretKey: "VLLM_API_KEY"

    #     # DGX Spark: 10x ARM Cortex-X925 cores, 128GB unified memory, GB10 Blackwell GPU
    #     requestCPU: 8
    #     requestMemory: "70Gi"
    #     requestGPU: 1

    #     # Persistent storage for model weights cache
    #     pvcStorage: "210Gi"
    #     pvcAccessMode:
    #       - ReadWriteOnce

    #     extraArgs:
    #       - "--extra-index-url"
    #       - "https://wheels.vllm.ai/gpt-oss/"
    #       - "--extra-index-url"
    #       - "https://download.pytorch.org/whl/nightly/cu128"
    #       - "--index-strategy"
    #       - "unsafe-best-match"

    #     nodeSelectorTerms:
    #       - matchExpressions:
    #           - key: nvidia.com/gpu.product
    #             operator: "In"
    #             values:
    #               - "NVIDIA-GB10"

    #     envFromSecret:
    #       name: hf-token

    # tolerations:
    #   - key: nvidia.com/gpu
    #     operator: Exists
    #     effect: NoSchedule

    - name: "qwen3-coder-next-fp8"
      repository: "nvcr.io/nvidia/vllm"
      tag: "26.01-py3"
      modelURL: "Qwen/Qwen3-Coder-Next-FP8"
      replicaCount: 1

      vllmApiKey:
        secretName: "vllm-apikey"
        secretKey: "VLLM_API_KEY"

      # DGX Spark: 10x ARM Cortex-X925 cores, 128GB unified memory, GB10 Blackwell GPU
      requestCPU: 8
      requestMemory: "100Gi"
      requestGPU: 1

      # Persistent storage for model weights cache
      pvcStorage: "82Gi"
      enableTool: true
      toolCallParser: "qwen3_coder"

      vllmConfig:
        gpuMemoryUtilization: 0.8
        enablePrefixCaching: true
        extraArgs:
          # - "--load-format"
          # - "fastsafetensors"
          - "--attention-backend"
          - "flashinfer"

      nodeSelectorTerms:
        - matchExpressions:
            - key: nvidia.com/gpu.product
              operator: "In"
              values:
                - "NVIDIA-GB10"

      envFromSecret:
        name: hf-token

  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule

  # Comment out these values once model is cached on PVC
  startupProbe:
    initialDelaySeconds: 180 # 10 min before first probe
    failureThreshold: 400 # 400 Ã— 10s = +1h of retries
    periodSeconds: 10
