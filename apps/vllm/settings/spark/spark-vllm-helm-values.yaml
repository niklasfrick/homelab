servingEngineSpec:
  labels:
    environment: "spark"
    release: "v1"

  modelSpec:
    # ============================================================================
    # Mistral Large 3 - Multimodal MoE Model (41B active / 675B total params)
    # Optimized for NVIDIA DGX Spark with 128GB unified memory
    # ============================================================================
    # - name: "mistral-large-3"
    #   repository: "nvcr.io/nvidia/vllm"
    #   tag: "25.11-py3"
    #   modelURL: "mistralai/Mistral-Large-3-675B-Instruct-2512-NVFP4"
    #   replicaCount: 1

    #   vllmApiKey:
    #     secretName: "vllm-apikey"
    #     secretKEy: "VLLM_API_KEY"

    #   # DGX Spark: 10x ARM Cortex-X925 cores, 128GB unified memory, GB10 Blackwell GPU
    #   requestCPU: 8
    #   requestMemory: "100Gi"
    #   requestGPU: 1

    #   # Shared memory for MoE expert routing and tensor operations
    #   shmSize: "16Gi"

    #   # Persistent storage for model weights cache
    #   pvcStorage: "450Gi"
    #   pvcAccessMode:
    #     - ReadWriteOnce

    #   vllmConfig:
    #     # Context length - MoE model supports up to 256k but we use conservative value
    #     # for DGX Spark unified memory to leave room for KV cache
    #     maxModelLen: 65536

    #     gpuMemoryUtilization: 0.9

    #     # Auto dtype - will use optimal precision on Blackwell
    #     dtype: "auto"

    #     # Single GB10 GPU on DGX Spark
    #     tensorParallelSize: 1

    #     # Chunked prefill improves TTFT for long prompts
    #     enableChunkedPrefill: true

    #     # Prefix caching improves throughput for repeated prefixes
    #     enablePrefixCaching: true

    #     # Extra args optimized for DGX Spark unified memory + MoE architecture
    #     extraArgs:
    #       - "--max-num-batched-tokens=32768"
    #       - "--tokenizer-mode=mistral"
    #       - "--config-format=mistral"
    #       - "--load-format=mistral"
    #       - "--enable-auto-tool-choice"
    #       - "--tool-call-parser=mistral"

    #   nodeSelectorTerms:
    #     - matchExpressions:
    #         - key: nvidia.com/gpu.product
    #           operator: "In"
    #           values:
    #             - "NVIDIA-GB10"

    #   envFromSecret:
    #     name: hf-token

    - name: "gpt-oss-120b"
      repository: "nvcr.io/nvidia/vllm"
      tag: "25.11-py3"
      modelURL: "openai/gpt-oss-120b"
      replicaCount: 1

      vllmApiKey:
        secretName: "vllm-apikey"
        secretKey: "VLLM_API_KEY"

      # DGX Spark: 10x ARM Cortex-X925 cores, 128GB unified memory, GB10 Blackwell GPU
      requestCPU: 8
      requestMemory: "70Gi"
      requestGPU: 1

      # Persistent storage for model weights cache
      pvcStorage: "210Gi"
      pvcAccessMode:
        - ReadWriteOnce

      extraArgs:
        - "--extra-index-url"
        - "https://wheels.vllm.ai/gpt-oss/"
        - "--extra-index-url"
        - "https://download.pytorch.org/whl/nightly/cu128"
        - "--index-strategy"
        - "unsafe-best-match"

      nodeSelectorTerms:
        - matchExpressions:
            - key: nvidia.com/gpu.product
              operator: "In"
              values:
                - "NVIDIA-GB10"

      envFromSecret:
        name: hf-token

  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule

  # Startup probe for large model loading
  # NOTE: 400+ GB model @ 200 Mbit/s ≈ 4.5 hours download + loading time
  # Comment out these values once model is cached on PVC
  # startupProbe:
  #   initialDelaySeconds: 1800 # 30 min before first probe
  #   failureThreshold: 1800 # 1800 × 10s = 5 hours of retries
  #   periodSeconds: 10 # Total: ~5.5 hours

  # # Liveness probe to detect hung processes
  # livenessProbe:
  #   initialDelaySeconds: 21600 # 6 hours - wait for startup to complete
  #   periodSeconds: 60
  #   failureThreshold: 5

  # # Readiness probe for traffic routing
  # readinessProbe:
  #   initialDelaySeconds: 21600 # 6 hours - wait for startup to complete
  #   periodSeconds: 30
  #   failureThreshold: 3
